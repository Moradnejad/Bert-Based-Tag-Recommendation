{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "papermill": {
          "duration": 0.02462,
          "end_time": "2021-01-06T14:24:23.259454",
          "exception": false,
          "start_time": "2021-01-06T14:24:23.234834",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": false,
          "start_time": "2021-01-06T14:24:23.272653",
          "status": "running"
        },
        "tags": [],
        "trusted": true
      },
      "cell_type": "code",
      "source": "pip install bert-tensorflow",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "trusted": true
      },
      "cell_type": "code",
      "source": "pip install bert-for-tf2",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "trusted": true
      },
      "cell_type": "code",
      "source": "pip install sentencepiece",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "trusted": true
      },
      "cell_type": "code",
      "source": "try:\n    %tensorflow_version 2.x\nexcept Exception:\n    pass\nimport tensorflow as tf\n\nimport tensorflow_hub as hub\n\nfrom tensorflow.keras import layers\nimport bert",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nimport gc\nimport os\nimport numpy as np\nimport collections\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom keras.callbacks import ModelCheckpoint\nimport random",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "trusted": true
      },
      "cell_type": "code",
      "source": "def transliterate(line):\n    cedilla2latin = [[u'Á', u'A'], [u'á', u'a'], [u'Č', u'C'], [u'č', u'c'], [u'Š', u'S'], [u'š', u's']]\n    tr = dict([(a[0], a[1]) for (a) in cedilla2latin])\n    new_line = \"\"\n    for letter in line:\n        if letter in tr:\n            new_line += tr[letter]\n        else:\n            new_line += letter\n    return new_line",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "trusted": true
      },
      "cell_type": "code",
      "source": "def text_cleaner(text,\n                 deep_clean=True,\n                 stem= True,\n                 stop_words=True,\n                 translite_rate=True):\n    rules = [\n        {r'>\\s+': u'>'},  # remove spaces after a tag opens or closes\n        {r'\\s+': u' '},  # replace consecutive spaces\n        {r'\\s*<br\\s*/?>\\s*': u'\\n'},  # newline after a <br>\n        {r'</(div)\\s*>\\s*': u'\\n'},  # newline after </p> and </div> and <h1/>...\n        {r'</(p|h\\d)\\s*>\\s*': u'\\n\\n'},  # newline after </p> and </div> and <h1/>...\n        {r'<head>.*<\\s*(/head|body)[^>]*>': u''},  # remove <head> to </head>\n        {r'<a\\s+href=\"([^\"]+)\"[^>]*>.*</a>': r'\\1'},  # show links instead of texts\n        {r'[ \\t]*<[^<]*?/?>': u''},  # remove remaining tags\n        {r'^\\s+': u''}  # remove spaces at the beginning\n\n    ]\n\n    if deep_clean:\n        text = text.replace(\".\", \"\")\n        text = text.replace(\"[\", \" \")\n        text = text.replace(\",\", \" \")\n        text = text.replace(\"]\", \" \")\n        text = text.replace(\"(\", \" \")\n        text = text.replace(\")\", \" \")\n        text = text.replace(\"\\\"\", \"\")\n        text = text.replace(\"-\", \" \")\n        text = text.replace(\"=\", \" \")\n        text = text.replace(\"?\", \" \")\n        text = text.replace(\"!\", \" \")\n\n        for rule in rules:\n            for (k, v) in rule.items():\n                regex = re.compile(k)\n                text = regex.sub(v, text)\n            text = text.rstrip()\n            text = text.strip()\n        text = text.replace('+', ' ').replace('.', ' ').replace(',', ' ').replace(':', ' ')\n        text = re.sub(\"(^|\\W)\\d+($|\\W)\", \" \", text)\n        if translite_rate:\n            text = transliterate(text)\n        if stem:\n            text = PorterStemmer().stem(text)\n        text = WordNetLemmatizer().lemmatize(text)\n        if stop_words:\n            stop_words = set(stopwords.words('english'))\n            word_tokens = word_tokenize(text)\n            text = [w for w in word_tokens if not w in stop_words]\n            text = ' '.join(str(e) for e in text)\n    else:\n        for rule in rules:\n            for (k, v) in rule.items():\n                regex = re.compile(k)\n                text = regex.sub(v, text)\n            text = text.rstrip()\n            text = text.strip()\n    return text.lower()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "trusted": true
      },
      "cell_type": "code",
      "source": "#make pre-train for bert\nBertTokenizer = bert.bert_tokenization.FullTokenizer\nbert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n                            trainable=False)\nvocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\nto_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = BertTokenizer(vocabulary_file, to_lower_case)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nfrom bs4 import BeautifulSoup as bs\nfrom collections import Counter\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nimport numpy as np\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport numpy as np\nimport nltk\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nimport re\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nnltk.download(\"stopwords\")\ncachedStopWords = stopwords.words(\"english\")\nfrom sklearn.model_selection import train_test_split\n\n\n#Function to convert html body of data to text\ndef convert_html_to_text(data):\n    soup = bs(data,'html.parser')\n    body = soup.get_text()\n    return body\n\n#Prepare data\nmytrain = pd.read_csv(\"/kaggle/input/freecode/freecode_data.csv\")\n\n#print(mytrain['Tags'])\n\nmytrain['Title'] = mytrain['title'].apply(text_cleaner)\n\nmytrain['Body'] = mytrain['question'].apply(text_cleaner)\n\nmytrain['Text'] = mytrain['Title'] + \"\\n\" + mytrain['Body']\n\n\n\n\n#Frequency of each tag\n\nvectorizer = CountVectorizer(tokenizer = lambda x: x.split())\ntag_bow = vectorizer.fit_transform(mytrain['tag'])\n\ntags = vectorizer.get_feature_names()\n\nfreq = tag_bow.sum(axis=0).A1\ntag_to_count_map = dict(zip(tags, freq))\n\nlist = []\nfor key, value in tag_to_count_map.items():\n  list.append([key, value]) \n\n\ntag_df = pd.DataFrame(list, columns=['tag', 'Counts'])\nprint(tag_df.head())\n\ntag_df_sorted = tag_df.sort_values(['Counts'], ascending=False)\n\n\nprint(\"{} tags are used more than 80 times\".format(tag_df_sorted[tag_df_sorted[\"Counts\"]>50]))\n\n\n\nmost_common_tag = tag_df_sorted[tag_df_sorted[\"Counts\"]>80].shape[0]\n\nprint(\"most_common_tag:\",most_common_tag)\n\n\nX = mytrain['Text'].tolist()\n#print(X[7])\n\n\n#prepare tags\nlist_of_tags = []\nfor item in mytrain['tag']:\n    temp = item.split(\" \")\n    for word in temp:\n        if not word in list_of_tags:\n            list_of_tags.append(word)\n            \n\nlist_of_all_tags = []\nfor item in mytrain['tag']:\n    temp = item.split(\" \")\n    for word in temp:\n        list_of_all_tags.append(word)\n\n\ncounts = Counter(list_of_all_tags)\n\nmost_occurs = [(k, v) for k, v in counts.items() if v >= 80] # todo: check this with Navid\nprint(\"most_occurs\", most_occurs)\n\ntags = []\nfor item in most_occurs:\n    tags.append(item[0])\n    #print(\"tags:\",tags)\n\n\n\n#print(\"tags:\",tags)\ny = []\nS=0\nfor item in mytrain['tag']:\n    self_tags = []\n    itemsplitted = item.split(\" \")\n    \n    for word in tags:\n        if word in itemsplitted:\n            self_tags.append(1)\n        else:\n            self_tags.append(0)\n    \n    values = np.array(self_tags)\n    \n    Y=all(values == 0)\n    if Y==True:\n        \n        del X[S]\n        S=S-1\n    else:\n        y.append(np.array(self_tags))\n    S=S+1    \n    \n\ny_list = []\nfor elem in y:\n    y_list.append(elem.tolist())\n    \ndef tokenize_data(data):\n    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(data))\n\n\n\ntokenized_data = [tokenize_data(data) for data in X]\n\n\ntokenized_data_train,tokenized_data_test,y_list_train, y_list_test = train_test_split(tokenized_data, y_list, test_size = 10000,random_state = 42)\nprint(\"Number of data points in training data :\", len(tokenized_data_train))\nprint(\"Number of data points in test data :\", len(tokenized_data_test))\n\nfor c,item in enumerate(y_list_test):\n    y_list_test[c] = np.array(item)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "trusted": true
      },
      "cell_type": "code",
      "source": "def column(matrix, i):\n    return [row[i] for row in matrix]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Function to show progress in consule :)\ndef progress(count, total, status=''):\n    bar_len = 60\n    filled_len = int(round(bar_len * count / float(total)))\n\n    percents = round(100.0 * count / float(total), 1)\n    bar = '=' * filled_len + '-' * (bar_len - filled_len)\n\n    sys.stdout.write('[%s] %s%s ...%s\\r' % (bar, percents, '%', status))\n    sys.stdout.flush()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "trusted": true
      },
      "cell_type": "code",
      "source": "#bert text model\nclass TEXT_MODEL(tf.keras.Model):\n    \n    def __init__(self,\n                 vocabulary_size,\n                 embedding_dimensions=128,\n                 cnn_filters=50,\n                 dnn_units=512,\n                 model_output_classes=2,\n                 dropout_rate=0.1,\n                 training=False,\n                 name=\"text_model\"):\n        super(TEXT_MODEL, self).__init__(name=name)\n        \n        self.embedding = layers.Embedding(vocabulary_size,\n                                          embedding_dimensions)\n        self.cnn_layer1 = layers.Conv1D(filters=cnn_filters,\n                                        kernel_size=2,\n                                        padding=\"valid\",\n                                        activation=\"relu\")\n        self.cnn_layer2 = layers.Conv1D(filters=cnn_filters,\n                                        kernel_size=3,\n                                        padding=\"valid\",\n                                        activation=\"relu\")\n        self.cnn_layer3 = layers.Conv1D(filters=cnn_filters,\n                                        kernel_size=4,\n                                        padding=\"valid\",\n                                        activation=\"relu\")\n\n                                    \n            \n\n        self.pool = layers.GlobalMaxPool1D()\n        \n        self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\n        self.dropout = layers.Dropout(rate=dropout_rate)\n        if model_output_classes == 2:\n            self.last_dense = layers.Dense(units=1,\n                                           activation=\"sigmoid\")\n        else:\n            self.last_dense = layers.Dense(units=model_output_classes,\n                                           activation=\"softmax\")\n    \n    def call(self, inputs, training):\n        l = self.embedding(inputs)\n        l_1 = self.cnn_layer1(l) \n        l_1 = self.pool(l_1) \n        l_2 = self.cnn_layer2(l) \n        l_2 = self.pool(l_2)\n        l_3 = self.cnn_layer3(l)\n        l_3 = self.pool(l_3) \n\n\n        \n        concatenated = tf.concat([l_1, l_2, l_3], axis=-1) # (batch_size, 3 * cnn_filters)\n        concatenated = self.dense_1(concatenated)\n        concatenated = self.dropout(concatenated, training)\n        model_output = self.last_dense(concatenated)\n        \n        return model_output",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "trusted": true
      },
      "cell_type": "code",
      "source": "import sys\nimport os\nimport nltk\nfrom nltk.corpus import reuters\nfrom sklearn.preprocessing import MultiLabelBinarizer\nimport numpy as np\nimport random\nimport math\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import hamming_loss\nimport statistics \n\n\nwhole_predictions = []\nwhole_real_predictions = []\nwhole_threshold_predictions = []\n\none=0\n\n#predict for each label individualy\n\nfor i in range(len(y_list_train[0])):\n    print(\"\\n\" + str(i)+\"\\'th label prediction started\")\n    count_zero=0\n    count_one=0\n    new_label=[]\n    new_tokenized_data_train=[]\n    label = column(y_list_train,i)\n    count_one=sum(label)\n    print(\"count_one\",count_one)\n    \n    for k in range(len(label)):\n        if count_zero< count_one and label[k]==0:\n            new_label.append(0)\n            new_tokenized_data_train.append(tokenized_data_train[k])\n            count_zero=count_zero+1\n        if label[k]==1:\n            new_label.append(1)\n            new_tokenized_data_train.append(tokenized_data_train[k])\n\n            \n    print(\"count_zero\",count_zero)        \n    data_with_len = [[data,new_label[j],len(data)]\n                     for j,data in enumerate(new_tokenized_data_train)]\n\n    data_with_len.sort(key=lambda x: x[2])\n    sorted_data_labels = [(data_lab[0], data_lab[1]) for data_lab in data_with_len]\n    processed_dataset = tf.data.Dataset.from_generator(lambda: sorted_data_labels, output_types=(tf.int32, tf.int32))\n    BATCH_SIZE = 32\n    batched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))\n    TOTAL_BATCHES = math.ceil(len(sorted_data_labels) / BATCH_SIZE)\n#     TEST_BATCHES = TOTAL_BATCHES // TOTAL_BATCHES\n#     batched_dataset.shuffle(TOTAL_BATCHES)\n#     test_data = batched_dataset.take(TEST_BATCHES)\n#     train_data = batched_dataset.skip(TEST_BATCHES)\n    train_data = TOTAL_BATCHES\n    \n   \n    VOCAB_LENGTH = len(tokenizer.vocab)\n    EMB_DIM = 260\n    CNN_FILTERS = 50\n    DNN_UNITS = 256\n    OUTPUT_CLASSES = 2\n\n    DROPOUT_RATE = 0.2\n\n    NB_EPOCHS = 6\n\n    text_model = TEXT_MODEL(vocabulary_size=VOCAB_LENGTH,\n                        embedding_dimensions=EMB_DIM,\n                        cnn_filters=CNN_FILTERS,\n                        dnn_units=DNN_UNITS,\n                        model_output_classes=OUTPUT_CLASSES,\n                        dropout_rate=DROPOUT_RATE)\n\n    if OUTPUT_CLASSES == 2:\n        text_model.compile(loss=\"binary_crossentropy\",\n                           optimizer=\"adam\",\n                           metrics=[\"acc\"])\n    else:\n        text_model.compile(loss=\"sparse_categorical_crossentropy\",\n                           optimizer=\"adam\",\n                           metrics=[\"sparse_categorical_acc\"])\n\n    text_model.fit(train_data, epochs=NB_EPOCHS)\n\n    self_label_predictions = []\n    self_threshold_predictions = []\n    self_label_real_values = []\n    print(\"Predicting \" + str(i) + \"th label...\")\n    \n    for e,item in enumerate(tokenized_data_test):\n        if e%2==0:\n            progress(e,len(tokenized_data_test))\n        res = text_model.predict([item])\n        self_label_real_values.append(res[0][0])\n      \n        if res[0][0] > 0.93:\n            self_threshold_predictions.append(res[0][0])\n        else :\n            self_threshold_predictions.append(0.0)\n\n    whole_threshold_predictions.append(self_threshold_predictions)\n    whole_real_predictions.append(self_label_real_values)\n    \n    \n\n\nwhole_threshold_predictions = list(map(list, zip(*whole_threshold_predictions)))\nwhole_real_predictions = list(map(list, zip(*whole_real_predictions)))\n\n\n\n\n        \n#@k Formulas :\nK_list= [3,5,10]\nfor U in range(len(K_list)):\n    K_tag_y_list_test = []\n    k = K_list[U]\n    sigma_recalls = 0\n    sigma_precisions = 0\n    sigma_f1score = 0\n\n    K_tag_y_list_test = y_list_test\n    for f in range(len(K_tag_y_list_test)):\n        progress(f,len(K_tag_y_list_test))\n        currentitem = np.array(whole_threshold_predictions[f])\n\n        top_k_indexes = (-currentitem).argsort()[:k]\n        \n        for C in top_k_indexes:\n            if whole_threshold_predictions[f][C] == 0.0 :\n                top_k_indexes = top_k_indexes[top_k_indexes != C]\n      \n        intercep = 0\n        for numb in top_k_indexes:\n            if K_tag_y_list_test[f][numb] == 1 :\n                intercep += 1\n        num_of_exists_tags = np.count_nonzero(K_tag_y_list_test[f] == 1)\n\n        if len(top_k_indexes) == 0 :\n            self_recall_k=0\n        elif len(top_k_indexes) >= num_of_exists_tags :\n            self_recall_k = intercep / num_of_exists_tags\n        elif len(top_k_indexes) < num_of_exists_tags :\n            self_recall_k = intercep / len(top_k_indexes)\n        if len(top_k_indexes)==0:\n             self_precisions_k=0 \n        else:    \n            self_precisions_k = intercep / len(top_k_indexes)\n        if self_precisions_k==0 and self_recall_k==0:\n            self_f1_score_k=0\n        else:    \n            self_f1_score_k = 2 * ((self_precisions_k*self_recall_k)/(self_precisions_k+self_recall_k))\n        sigma_recalls += self_recall_k\n        sigma_precisions += self_precisions_k\n        sigma_f1score += self_f1_score_k\n\n    recall_k = sigma_recalls / len(K_tag_y_list_test)\n    precisions_k = sigma_precisions / len(K_tag_y_list_test)\n    f1score_k = sigma_f1score / len(K_tag_y_list_test)\n    print(\"\\n\")\n    print(\"Recall@\"+ str(K_list[U])+\" = \" + str(recall_k))\n    print(\"Precision@\"+ str(K_list[U])+\" = \" + str(precisions_k))\n    print(\"f1score@\"+ str(K_list[U])+\" = \" + str(f1score_k))\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "papermill": {
      "duration": null,
      "end_time": null,
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-01-06T14:24:18.249969",
      "version": "1.2.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
