{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.02462,
     "end_time": "2021-01-06T14:24:23.259454",
     "exception": false,
     "start_time": "2021-01-06T14:24:23.234834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2021-01-06T14:24:23.272653",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install bert-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install bert-for-tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import gc\n",
    "import os\n",
    "import numpy as np\n",
    "import collections\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transliterate(line):\n",
    "    cedilla2latin = [[u'Á', u'A'], [u'á', u'a'], [u'Č', u'C'], [u'č', u'c'], [u'Š', u'S'], [u'š', u's']]\n",
    "    tr = dict([(a[0], a[1]) for (a) in cedilla2latin])\n",
    "    new_line = \"\"\n",
    "    for letter in line:\n",
    "        if letter in tr:\n",
    "            new_line += tr[letter]\n",
    "        else:\n",
    "            new_line += letter\n",
    "    return new_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_cleaner(text,\n",
    "                 deep_clean=True,\n",
    "                 stem= True,\n",
    "                 stop_words=True,\n",
    "                 translite_rate=True):\n",
    "    rules = [\n",
    "        {r'>\\s+': u'>'},  # remove spaces after a tag opens or closes\n",
    "        {r'\\s+': u' '},  # replace consecutive spaces\n",
    "        {r'\\s*<br\\s*/?>\\s*': u'\\n'},  # newline after a <br>\n",
    "        {r'</(div)\\s*>\\s*': u'\\n'},  # newline after </p> and </div> and <h1/>...\n",
    "        {r'</(p|h\\d)\\s*>\\s*': u'\\n\\n'},  # newline after </p> and </div> and <h1/>...\n",
    "        {r'<head>.*<\\s*(/head|body)[^>]*>': u''},  # remove <head> to </head>\n",
    "        {r'<a\\s+href=\"([^\"]+)\"[^>]*>.*</a>': r'\\1'},  # show links instead of texts\n",
    "        {r'[ \\t]*<[^<]*?/?>': u''},  # remove remaining tags\n",
    "        {r'^\\s+': u''}  # remove spaces at the beginning\n",
    "\n",
    "    ]\n",
    "\n",
    "    if deep_clean:\n",
    "        text = text.replace(\".\", \"\")\n",
    "        text = text.replace(\"[\", \" \")\n",
    "        text = text.replace(\",\", \" \")\n",
    "        text = text.replace(\"]\", \" \")\n",
    "        text = text.replace(\"(\", \" \")\n",
    "        text = text.replace(\")\", \" \")\n",
    "        text = text.replace(\"\\\"\", \"\")\n",
    "        text = text.replace(\"-\", \" \")\n",
    "        text = text.replace(\"=\", \" \")\n",
    "        text = text.replace(\"?\", \" \")\n",
    "        text = text.replace(\"!\", \" \")\n",
    "\n",
    "        for rule in rules:\n",
    "            for (k, v) in rule.items():\n",
    "                regex = re.compile(k)\n",
    "                text = regex.sub(v, text)\n",
    "            text = text.rstrip()\n",
    "            text = text.strip()\n",
    "        text = text.replace('+', ' ').replace('.', ' ').replace(',', ' ').replace(':', ' ')\n",
    "        text = re.sub(\"(^|\\W)\\d+($|\\W)\", \" \", text)\n",
    "        if translite_rate:\n",
    "            text = transliterate(text)\n",
    "        if stem:\n",
    "            text = PorterStemmer().stem(text)\n",
    "        text = WordNetLemmatizer().lemmatize(text)\n",
    "        if stop_words:\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            word_tokens = word_tokenize(text)\n",
    "            text = [w for w in word_tokens if not w in stop_words]\n",
    "            text = ' '.join(str(e) for e in text)\n",
    "    else:\n",
    "        for rule in rules:\n",
    "            for (k, v) in rule.items():\n",
    "                regex = re.compile(k)\n",
    "                text = regex.sub(v, text)\n",
    "            text = text.rstrip()\n",
    "            text = text.strip()\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#make pre-train for bert\n",
    "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=False)\n",
    "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from collections import Counter\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "nltk.download(\"stopwords\")\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#Function to convert html body of data to text\n",
    "def convert_html_to_text(data):\n",
    "    soup = bs(data,'html.parser')\n",
    "    body = soup.get_text()\n",
    "    return body\n",
    "\n",
    "#Prepare data\n",
    "mytrain = pd.read_csv(\"/kaggle/input/freecode/freecode_data.csv\")\n",
    "\n",
    "#print(mytrain['Tags'])\n",
    "\n",
    "mytrain['Title'] = mytrain['title'].apply(text_cleaner)\n",
    "\n",
    "mytrain['Body'] = mytrain['question'].apply(text_cleaner)\n",
    "\n",
    "mytrain['Text'] = mytrain['Title'] + \"\\n\" + mytrain['Body']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Frequency of each tag\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer = lambda x: x.split())\n",
    "tag_bow = vectorizer.fit_transform(mytrain['tag'])\n",
    "\n",
    "tags = vectorizer.get_feature_names()\n",
    "\n",
    "freq = tag_bow.sum(axis=0).A1\n",
    "tag_to_count_map = dict(zip(tags, freq))\n",
    "\n",
    "list = []\n",
    "for key, value in tag_to_count_map.items():\n",
    "  list.append([key, value]) \n",
    "\n",
    "\n",
    "tag_df = pd.DataFrame(list, columns=['tag', 'Counts'])\n",
    "print(tag_df.head())\n",
    "\n",
    "tag_df_sorted = tag_df.sort_values(['Counts'], ascending=False)\n",
    "\n",
    "\n",
    "print(\"{} tags are used more than 80 times\".format(tag_df_sorted[tag_df_sorted[\"Counts\"]>50]))\n",
    "\n",
    "\n",
    "\n",
    "most_common_tag = tag_df_sorted[tag_df_sorted[\"Counts\"]>80].shape[0]\n",
    "\n",
    "print(\"most_common_tag:\",most_common_tag)\n",
    "\n",
    "\n",
    "X = mytrain['Text'].tolist()\n",
    "#print(X[7])\n",
    "\n",
    "\n",
    "#prepare tags\n",
    "list_of_tags = []\n",
    "for item in mytrain['tag']:\n",
    "    temp = item.split(\" \")\n",
    "    for word in temp:\n",
    "        if not word in list_of_tags:\n",
    "            list_of_tags.append(word)\n",
    "            \n",
    "\n",
    "list_of_all_tags = []\n",
    "for item in mytrain['tag']:\n",
    "    temp = item.split(\" \")\n",
    "    for word in temp:\n",
    "        list_of_all_tags.append(word)\n",
    "\n",
    "\n",
    "counts = Counter(list_of_all_tags)\n",
    "\n",
    "\n",
    "print(\"most_occurs\",most_occurs)\n",
    "\n",
    "tags = []\n",
    "for item in most_occurs:\n",
    "    tags.append(item[0])\n",
    "    #print(\"tags:\",tags)\n",
    "\n",
    "\n",
    "\n",
    "#print(\"tags:\",tags)\n",
    "y = []\n",
    "S=0\n",
    "for item in mytrain['tag']:\n",
    "    self_tags = []\n",
    "    itemsplitted = item.split(\" \")\n",
    "    \n",
    "    for word in tags:\n",
    "        if word in itemsplitted:\n",
    "            self_tags.append(1)\n",
    "        else:\n",
    "            self_tags.append(0)\n",
    "    \n",
    "    values = np.array(self_tags)\n",
    "    \n",
    "    Y=all(values == 0)\n",
    "    if Y==True:\n",
    "        \n",
    "        del X[S]\n",
    "        S=S-1\n",
    "    else:\n",
    "        y.append(np.array(self_tags))\n",
    "    S=S+1    \n",
    "    \n",
    "\n",
    "y_list = []\n",
    "for elem in y:\n",
    "    y_list.append(elem.tolist())\n",
    "    \n",
    "def tokenize_data(data):\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(data))\n",
    "\n",
    "\n",
    "\n",
    "tokenized_data = [tokenize_data(data) for data in X]\n",
    "\n",
    "\n",
    "tokenized_data_train,tokenized_data_test,y_list_train, y_list_test = train_test_split(tokenized_data, y_list, test_size = 10000,random_state = 42)\n",
    "print(\"Number of data points in training data :\", len(tokenized_data_train))\n",
    "print(\"Number of data points in test data :\", len(tokenized_data_test))\n",
    "\n",
    "for c,item in enumerate(y_list_test):\n",
    "    y_list_test[c] = np.array(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def column(matrix, i):\n",
    "    return [row[i] for row in matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Function to show progress in consule :)\n",
    "def progress(count, total, status=''):\n",
    "    bar_len = 60\n",
    "    filled_len = int(round(bar_len * count / float(total)))\n",
    "\n",
    "    percents = round(100.0 * count / float(total), 1)\n",
    "    bar = '=' * filled_len + '-' * (bar_len - filled_len)\n",
    "\n",
    "    sys.stdout.write('[%s] %s%s ...%s\\r' % (bar, percents, '%', status))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#bert text model\n",
    "class TEXT_MODEL(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocabulary_size,\n",
    "                 embedding_dimensions=128,\n",
    "                 cnn_filters=50,\n",
    "                 dnn_units=512,\n",
    "                 model_output_classes=2,\n",
    "                 dropout_rate=0.1,\n",
    "                 training=False,\n",
    "                 name=\"text_model\"):\n",
    "        super(TEXT_MODEL, self).__init__(name=name)\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocabulary_size,\n",
    "                                          embedding_dimensions)\n",
    "        self.cnn_layer1 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=2,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.cnn_layer2 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=3,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.cnn_layer3 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=4,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "\n",
    "                                    \n",
    "            \n",
    "\n",
    "        self.pool = layers.GlobalMaxPool1D()\n",
    "        \n",
    "        self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        if model_output_classes == 2:\n",
    "            self.last_dense = layers.Dense(units=1,\n",
    "                                           activation=\"sigmoid\")\n",
    "        else:\n",
    "            self.last_dense = layers.Dense(units=model_output_classes,\n",
    "                                           activation=\"softmax\")\n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "        l = self.embedding(inputs)\n",
    "        l_1 = self.cnn_layer1(l) \n",
    "        l_1 = self.pool(l_1) \n",
    "        l_2 = self.cnn_layer2(l) \n",
    "        l_2 = self.pool(l_2)\n",
    "        l_3 = self.cnn_layer3(l)\n",
    "        l_3 = self.pool(l_3) \n",
    "\n",
    "\n",
    "        \n",
    "        concatenated = tf.concat([l_1, l_2, l_3], axis=-1) # (batch_size, 3 * cnn_filters)\n",
    "        concatenated = self.dense_1(concatenated)\n",
    "        concatenated = self.dropout(concatenated, training)\n",
    "        model_output = self.last_dense(concatenated)\n",
    "        \n",
    "        return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import hamming_loss\n",
    "import statistics \n",
    "\n",
    "\n",
    "whole_predictions = []\n",
    "whole_real_predictions = []\n",
    "whole_threshold_predictions = []\n",
    "\n",
    "one=0\n",
    "\n",
    "#predict for each label individualy\n",
    "\n",
    "for i in range(len(y_list_train[0])):\n",
    "    print(\"\\n\" + str(i)+\"\\'th label prediction started\")\n",
    "    count_zero=0\n",
    "    count_one=0\n",
    "    new_label=[]\n",
    "    new_tokenized_data_train=[]\n",
    "    label = column(y_list_train,i)\n",
    "    count_one=sum(label)\n",
    "    print(\"count_one\",count_one)\n",
    "    \n",
    "    for k in range(len(label)):\n",
    "        if count_zero< count_one and label[k]==0:\n",
    "            new_label.append(0)\n",
    "            new_tokenized_data_train.append(tokenized_data_train[k])\n",
    "            count_zero=count_zero+1\n",
    "        if label[k]==1:\n",
    "            new_label.append(1)\n",
    "            new_tokenized_data_train.append(tokenized_data_train[k])\n",
    "\n",
    "            \n",
    "    print(\"count_zero\",count_zero)        \n",
    "    data_with_len = [[data,new_label[j],len(data)]\n",
    "                     for j,data in enumerate(new_tokenized_data_train)]\n",
    "\n",
    "    data_with_len.sort(key=lambda x: x[2])\n",
    "    sorted_data_labels = [(data_lab[0], data_lab[1]) for data_lab in data_with_len]\n",
    "    processed_dataset = tf.data.Dataset.from_generator(lambda: sorted_data_labels, output_types=(tf.int32, tf.int32))\n",
    "    BATCH_SIZE = 32\n",
    "    batched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))\n",
    "    TOTAL_BATCHES = math.ceil(len(sorted_data_labels) / BATCH_SIZE)\n",
    "    TEST_BATCHES = TOTAL_BATCHES // TOTAL_BATCHES\n",
    "    batched_dataset.shuffle(TOTAL_BATCHES)\n",
    "    test_data = batched_dataset.take(TEST_BATCHES)\n",
    "    train_data = batched_dataset.skip(TEST_BATCHES)\n",
    "    \n",
    "   \n",
    "    VOCAB_LENGTH = len(tokenizer.vocab)\n",
    "    EMB_DIM = 260\n",
    "    CNN_FILTERS = 50\n",
    "    DNN_UNITS = 256\n",
    "    OUTPUT_CLASSES = 2\n",
    "\n",
    "    DROPOUT_RATE = 0.2\n",
    "\n",
    "    NB_EPOCHS = 6\n",
    "\n",
    "    text_model = TEXT_MODEL(vocabulary_size=VOCAB_LENGTH,\n",
    "                        embedding_dimensions=EMB_DIM,\n",
    "                        cnn_filters=CNN_FILTERS,\n",
    "                        dnn_units=DNN_UNITS,\n",
    "                        model_output_classes=OUTPUT_CLASSES,\n",
    "                        dropout_rate=DROPOUT_RATE)\n",
    "\n",
    "    if OUTPUT_CLASSES == 2:\n",
    "        text_model.compile(loss=\"binary_crossentropy\",\n",
    "                           optimizer=\"adam\",\n",
    "                           metrics=[\"acc\"])\n",
    "    else:\n",
    "        text_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                           optimizer=\"adam\",\n",
    "                           metrics=[\"sparse_categorical_acc\"])\n",
    "\n",
    "    text_model.fit(train_data, epochs=NB_EPOCHS)\n",
    "\n",
    "    self_label_predictions = []\n",
    "    self_threshold_predictions = []\n",
    "    self_label_real_values = []\n",
    "    print(\"Predicting \" + str(i) + \"th label...\")\n",
    "    \n",
    "    for e,item in enumerate(tokenized_data_test):\n",
    "        if e%2==0:\n",
    "            progress(e,len(tokenized_data_test))\n",
    "        res = text_model.predict([item])\n",
    "        self_label_real_values.append(res[0][0])\n",
    "      \n",
    "        if res[0][0] > 0.93:\n",
    "            self_threshold_predictions.append(res[0][0])\n",
    "        else :\n",
    "            self_threshold_predictions.append(0.0)\n",
    "\n",
    "    whole_threshold_predictions.append(self_threshold_predictions)\n",
    "    whole_real_predictions.append(self_label_real_values)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "whole_threshold_predictions = list(map(list, zip(*whole_threshold_predictions)))\n",
    "whole_real_predictions = list(map(list, zip(*whole_real_predictions)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "#@k Formulas :\n",
    "K_list= [3,5,10]\n",
    "for U in range(len(K_list)):\n",
    "    K_tag_y_list_test = []\n",
    "    k = K_list[U]\n",
    "    sigma_recalls = 0\n",
    "    sigma_precisions = 0\n",
    "    sigma_f1score = 0\n",
    "\n",
    "    K_tag_y_list_test = y_list_test\n",
    "    for f in range(len(K_tag_y_list_test)):\n",
    "        progress(f,len(K_tag_y_list_test))\n",
    "        currentitem = np.array(whole_threshold_predictions[f])\n",
    "\n",
    "        top_k_indexes = (-currentitem).argsort()[:k]\n",
    "        \n",
    "        for C in top_k_indexes:\n",
    "            if whole_threshold_predictions[f][C] == 0.0 :\n",
    "                top_k_indexes = top_k_indexes[top_k_indexes != C]\n",
    "      \n",
    "        intercep = 0\n",
    "        for numb in top_k_indexes:\n",
    "            if K_tag_y_list_test[f][numb] == 1 :\n",
    "                intercep += 1\n",
    "        num_of_exists_tags = np.count_nonzero(K_tag_y_list_test[f] == 1)\n",
    "\n",
    "        if len(top_k_indexes) == 0 :\n",
    "            self_recall_k=0\n",
    "        elif len(top_k_indexes) >= num_of_exists_tags :\n",
    "            self_recall_k = intercep / num_of_exists_tags\n",
    "        elif len(top_k_indexes) < num_of_exists_tags :\n",
    "            self_recall_k = intercep / len(top_k_indexes)\n",
    "        if len(top_k_indexes)==0:\n",
    "             self_precisions_k=0 \n",
    "        else:    \n",
    "            self_precisions_k = intercep / len(top_k_indexes)\n",
    "        if self_precisions_k==0 and self_recall_k==0:\n",
    "            self_f1_score_k=0\n",
    "        else:    \n",
    "            self_f1_score_k = 2 * ((self_precisions_k*self_recall_k)/(self_precisions_k+self_recall_k))\n",
    "        sigma_recalls += self_recall_k\n",
    "        sigma_precisions += self_precisions_k\n",
    "        sigma_f1score += self_f1_score_k\n",
    "\n",
    "    recall_k = sigma_recalls / len(K_tag_y_list_test)\n",
    "    precisions_k = sigma_precisions / len(K_tag_y_list_test)\n",
    "    f1score_k = sigma_f1score / len(K_tag_y_list_test)\n",
    "    print(\"\\n\")\n",
    "    print(\"Recall@\"+ str(K_list[U])+\" = \" + str(recall_k))\n",
    "    print(\"Precision@\"+ str(K_list[U])+\" = \" + str(precisions_k))\n",
    "    print(\"f1score@\"+ str(K_list[U])+\" = \" + str(f1score_k))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "papermill": {
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-01-06T14:24:18.249969",
   "version": "1.2.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}